---
layout: article
title:  "「VIDEO」 视频分类概述"
date:   2019-05-16 15:06:40 +0800
key: video-classify-survey-20190516
aside:
  toc: true
category: [AI, video, video_classification]
---
`video understanding` · `video classification` · `action recognition`   
>知道这段视频在做什么：行为识别就是对时域预先分割好的序列判定其所属行为动作的类型，即“读懂行为”；   

<!--more-->

# 1 简介
图 1：发展路线图   

视频理解方向包含众多的子研究方向，以 CVPR 组织的 ACTIVITYNET 为例，2017年总共有5个Task被提出：  
- Task1：**未修剪视频分类**(Untrimmed Video Classification)；这个有点类似于图像的分类，未修剪的视频中通常含有多个动作，而且视频很长。有许多动作或许都不是我们所关注的。所以这里提出的Task就是希望通过对输入的长视频进行全局分析，然后软分类到多个类别；   
- Task2：**修剪视频识别**(Trimmed Action Recognition)；这个在计算机视觉领域已经研究多年，给出一段只包含一个动作的修剪视频，要求给视频分类；   
- Task3：**时序行为提名**(Temporal Action Proposal)；这个同样类似于图像目标检测任务中的候选框提取。在一段长视频中通常含有很多动作，这个任务就是从视频中找出可能含有动作的视频段；   
- Task4：**时序行为定位**(Temporal Action Localization)；相比于上面的时序行为提名而言，时序行为定位于我们常说的目标检测一致。要求从视频中找到可能存在行为的视频段，并且给视频段分类；   
- Task5：**密集行为描述**(Dense-Captioning Events)；之所以称为密集行为描述，主要是因为该任务要求在时序行为定位(检测)的基础上进行视频行为描述。也就是说，该任务需要将一段未修剪的视频进行时序行为定位得到许多包含行为的视频段后，对该视频段进行行为描述；比如：man playing a piano    
而该调研报告主要聚焦于行为识别和行为检测。也就是上述任务描述中的Task2和Task4；    

## 1.1 现状
Task 1：未修剪视频分类 State-of-art   

| 年份 | 研究员 | 指标 | 研究员 | 指标 |
| --- | --- | --- | --- | --- |
| 2017 | Jiankang Deng（伦敦帝国 IBUG） | 0.0883 | Yuanjun Xiong（CUHK） | 0.097 |

Task 2：已修剪视频分类 State-of-art   

| 年份 | 研究员 | 指标 | 研究员 | 指标 |
| --- | --- | --- | --- | --- |
| 2017 | Chuang GAN（清华） | 0.1239 | Yuanjun Xiong（CUHK） | 0.139 |


Task 3：时序行为提取 State-of-art   

| 年份 | 研究员 | 指标 | 研究员 | 指标 |
| --- | --- | --- | --- | --- |
| 2017 | Tianwei Lin（上交） | 0.6626 | Vendetta V（中科大） | 0.6614 |


Task 4：时序行为定位 State-of-art   

| 年份 | 研究员 | 指标 | 研究员 | 指标 |
| --- | --- | --- | --- | --- |
| 2017 | Tianwei Lin（上交） | 0.35 | Yuanjun Xiong（CUHK） | 0.3186 |


Task 5：行为描述 State-of-art   

| 年份 | 研究员 | 指标 | 研究员 | 指标 |
| --- | --- | --- | --- | --- |
| 2017 | Peter Li（） | 0.129 | Ting Yao（MSRA） | 0.128 |


# 2 发展
## 2.1 算法分类

### 2.1.1 按网络结构分
1. **传统方法**    
在深度学习之前，iDT(improved Dense Trajectories)方法是最经典的一种方法；   

1. **单帧 CNN**    
网络单独处理每一帧，然后用全连接融合特征；当取到和主题无关的图像时，会让分类器迷惑；因此学习时域特性是提高准确度的主要因素；当然，这是在运动性较强的视频上，如果是静止的，就只能靠图像特征了；    

1. **扩展 CNN**    
在CNN框架中寻找时间域上的某个模式来表达局部运动信息；   
比单帧 CNN 提升 2 个百分点，特别是在运动丰富的视频提升更多，从而证明了该方法的有效性；    

1. **双流法**    
一个主流方法，最早是 VGG 团队在 NIPS 上提出来的；在这之前也有人尝试用深度学习来处理行为识别，例如[李飞飞团队](https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/42455.pdf)，通过叠加视频多帧输入到网络中进行学习，但是不幸的是这种方法比手动提取特征更加糟糕；当 **Two-Stream CNN** 出来后才意味着深度学习在行为识别中迈出了重大的一步；    
基本思路是一部分处理 RGB 图像（单帧或是多帧堆叠），一部分处理光流图像；最终联合训练进行分类； **TSN** 是在 Two-Stream CNN 的改进版，可以处理长时间的视频，目前双流法的网络基本上都是以 TSN 为基础网络；      

1. **C3D**    
另一个主流方法，即 3D CNN，普遍比双流法低几个百分点；但是速度快，且网络简洁，训练基本上是端到端的；  

1. **RNN**    
有潜力的方法，基本思想是用 LSTM 对每一帧的 CNN 最后一层的激活在时间轴上进行整合；这里，它没有用 CNN 全连接层后的最后特征进行融合，是因为全连接层后的高层特征进行池化已经丢失了空间特征在时间轴上的信息；   
目前最新进展是中科院深圳先进院乔宇老师的工作 RPAN；

>行为识别目前还是视频理解方向的热点，而且至今为止也没有得到很好的解决；由于视频中目标复杂，场景复杂，所以单纯的 Two-Stream和 C3D 方法表现得都不太如意；RPAN 中引入了姿态监督的机制，或许能提高视频分类的效果；    

### 2.1.2 按输入数据分
- 空间   
空间网络主要捕捉视频帧中重要的物体特征；目前大部分公开数据集仅用单帧图像就可以完成对视频的分类，那么，在这种情况下，网络的输入就存在着很大的冗余，且可能引入额外的噪声；目前主要是从视频中提取关键帧来提升分类的水平；   
- 时间   
相比之下，拥有更多关注；光流的计算耗费算力和时间（有些模型中几乎占据了 90% 的训练时间），其次，光流也未必是最优的运动特征；   

### 2.1.3 按时空融合分
connection 主要是指双流网络中时空信息的交互；  
- 一种是单个网络内部各层之间的交互，如 ResNet/Inception；   
- 一种是双流网络之间的交互，包括不同 fusion 方式的探索；  

目前值得考虑的是参照 ResNet 的结构，连接双流网络；另外 attention 机制也常用于时空信息的融合；     



## 2.2 性能对比

| 模型 |  UCF-101 | HMDB51 | Kinetics | activity-net | 1M sport |
| --- | --- | --- | --- | --- | --- |
| T3D |  |  |  |  |  |
| key-frame mining | 93.1 | 63.3 |  |  |  |
| ST-ResNet | 93.4 | 70.3 |  |  |  |
| TSN | 94.2 | 69.4 |  |  |  |
| ST-Multipier | 94.2 | 68.9 |  |  |  |
| ST-Pyramid Net | 94.6 | 68.9 |  |  |  |
| ActionVLAD | 94.9 | 69.8 |  |  |  |
| DOVF | 94.9 | 71.7 |  |  |  |
| I3D | 95.6 | 74.8 |  |  |  |
| TLE | 95.6 | 71.1 |  |  |  |
| S3D | 96.8 | 75.9 |  |  |  |
| Temporal Pyramid Pooling | 98 | 82.1 |  |  |  |
| Two-Stream I3D | 98 | 80.7 |  |  |  |

# 3 研究难点
行为识别处理的是视频，所以相对于图像分类来说多了一个需要处理的时序维度；行为识别还有一个痛点是视频段长度不一，而且开放环境下视频中存在多尺度、多目标、摄像机移动等众多的问题。这些问题都是导致行为识别还未能实用化的重要原因；   

# 4 思考

近年来对运动信息的关注逐渐上升，指责行为识别过度依赖背景和外貌特征，而缺少对运动本身的建模，但是，事实上，运动既不是名词，也不应该是动词，而应该是动词+名词的形式，例如：play+basketball，其中空间信息承担名词部分，时间信息承担动词部分；所以，虽然应该加大的时间信息的关注，但也不可忽略空间特征的重要作用；    

- 在 motion 特征被理解之前，双流网络可能仍然是主流；   
- 时空信息交互仍然有探索的余地，也是最有可能发论文的重点领域；   
- 输入方面，除了数据增强方法，替代光流的特征很值得期待；   

-------------------  
 [End]()
{:.warning}  


# 附录
## A 术语

| 英文 | 中文 | 英文 | 中文 |   
| --- | --- | --- | --- |
| Video classification | 视频分类 | Video captioning | 视频描述生成 |
| Clip | 片段 | Volume | 域 |
| Frame | 帧 | spatial-temporal | 时空 |
| appearance | 表观 | video representations | 视频描述 |
| feature representation | 特征描述 | descriptor | 描述符 |
| frame-level | 帧层 | video-level | 视频层 |
| segmentation | 分割 | hand-crafted feature | 人工设计的特征 |
| state-of-the-art | 最高水平 | off-the-shelf | 现有 |
| Untrimmed Video Classification | 视频多分类 | sequence | 序列 |
| Trimmed Activity Classification | 视频单分类 | Motion Detection and Tracking | 运动检测与跟踪 |
| 边缘 | edge | 图像分割 | Image segmentation |
| 纹理特征提取 | feature extraction | 局部特征 | local features |
| 人工标注 | Ground-truth | 自动标注 | Automatic Annotation |
| 图像表示 | Image Representation | | |


## B 参考文献
1. 王弗兰克. Model for Video Understanding【2】[EB/OL]. <https://zhuanlan.zhihu.com/p/33040925>. 2018-01-18/2019-05-17.    

---
layout: article
title:  "「DL」 GAN 的训练技巧"
date:   2019-04-09 15:48:40 +0800
key: gan-tricks-20190409
aside:
  toc: true
category: [AI, DL, GAN]
sidebar:
  nav: GAN
---

>

<!--more-->


1. **稳定GAN训练的技巧**  
如上所述，GAN 训练过程十分不稳定，可以通过一些技巧来文帝给你训练：  
- **Feature matching**: 方法很简单，使用判别器某一层的特征替换原始 GAN Loss 中的输出；即最小化：生成图片通过判别器的特征和真实图片通过判别器得到的特征之间的距离；  
- **标签平滑**：GAN 训练中的标签非 0 即 1，这使得判别器预测出来的 confidence 倾向于更高的值；使用标签平滑可以缓解该问题；具体来说，就是把标签1替换为0.8~2.0之间的随机数；  
- **谱归一化**：WGAN 和 Improve WGAN 通过施加 Lipschitz 条件来约束优化过程，谱归一化则是对判别器的每一层都施加 Lipschitz 约束，但是谱归一化相比于 Improve WGAN 计算效率要高一些；  
- **PatchGAN**：准确来说 PatchGAN 并不是用于稳定训练，但这个技术被广泛用于图像翻译当中，PatchGAN 相当于对图像的每一个小 Patch 进行判别，这样可以使得生成器生成更加锐利清晰的边缘；具体做法是这样的：假设输入一张 256x256 的图像到判别器，输出的是一个4x4的 confidence map，confidence map中每一个像素值代表当前 patch 是真实图像的置信度，即为 PatchGAN；当前图像 patch 的大小就是感受野的大小，最后将所有 Patch 的 Loss 求平均作为最终的 Loss；  

1. **解决模式崩溃**   
** 改进目标函数**  
为了避免前面提到的由于优化 maxmin 导致 mode 跳来跳去的问题，UnrolledGAN 采用修改生成器 loss 来解决；具体而言，UnrolledGAN 在更新生成器时更新k次生成器，参考的 Loss 不是某一次的 loss，是判别器后面 k 次迭代的 loss；注意，判别器后面 k 次迭代不更新自己的参数，只计算 loss 用于更新生成器；这种方式使得生成器考虑到了后面 k 次判别器的变化情况，避免在不同 mode 之间切换导致的模式崩溃问题；此处务必和迭代k次生成器，然后迭代 1 次判别器区分开；DRAGAN 则引入博弈论中的无后悔算法，改造其 loss 以解决 mode collapse 问题；前文所述的 EBGAN 则是加入 VAE 的重构误差以解决 mode collapse；    

**改进网络结构**  
**MAD-GAN**（Multi agent diverse GAN）采用多个生成器，一个判别器以保障样本生成的多样性；相比于普通 GAN，多了几个生成器，且在 loss 设计的时候，加入一个正则项；正则项使用余弦距离惩罚三个生成器生成样本的一致性；   
**MRGAN**则添加了一个判别器来惩罚生成样本的 mode collapse 问题；输入样本 x 通过一个 Encoder 编码为隐变量 $E(x)$ ，然后隐变量被 Generator 重构，训练时，Loss 有三个；$D_M$ 和 $R$（重构误差）用于指导生成 real-like 的样本；而 $D_D$ 则对 $E(x)$ 和 $z$ 生成的样本进行判别，显然二者生成样本都是 fake samples，所以这个判别器主要用于判断生成的样本是否具有多样性，即是否出现 mode collapse；   

**Mini-batch Discrimination**  
Mini-batch discrimination 在判别器的中间层建立一个 mini-batch layer 用于计算基于 L1 距离的样本统计量，通过建立该统计量去判别一个 batch 内某个样本与其他样本有多接近；这个信息可以被判别器利用到，从而甄别出哪些缺乏多样性的样本；对生成器而言，则要试图生成具有多样性的样本；  

-------------------  
 End
{:.warning}  


# 附录
## A 参考资料
[1]. soumith. How to Train a GAN? Tips and tricks to make GANs work[EB/OL]. <https://github.com/soumith/ganhacks>. 2016-12-17/2019-04-09.     
